{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AirQo_Prototype_ver_4.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7i3mZyCHapt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Required to download the current forecast data files. S\n",
        "\n",
        "# import gdown\n",
        "\n",
        "# url = 'https://drive.google.com/uc?id=1u5j6P2M5e97a0Olr0P3uTxbi7hiDtRdc'\n",
        "# output = 'Air_Quality_Data.zip'\n",
        "# gdown.download(url, output, quiet=False)\n",
        "\n",
        "# url = 'https://drive.google.com/uc?id=1MmdGnTtZ2HXIjsu98QcMcXJZxpNoWq0E'\n",
        "# output = 'Weather_Data.zip'\n",
        "# gdown.download(url, output, quiet=False)\n",
        "\n",
        "# !unzip -q Air_Quality_Data.zip\n",
        "# !unzip -q Weather_Data.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u6FPiXfevf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import os\n",
        "import joblib\n",
        "from joblib import Parallel, delayed"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN08ChRcH8hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocess_forecast_data(FORECAST_DATA_PATH):\n",
        "\n",
        "  forecast_data = pd.read_csv('Zindi_PM2_5_forecast_data.csv', usecols = ['created_at','channel_id', 'pm2_5'])\n",
        "  forecast_data['created_at'] = forecast_data['created_at'].apply(lambda x: x.replace(r'+03:00', ''))\n",
        "  forecast_data['created_at'] = pd.to_datetime(forecast_data['created_at'], format='%Y-%m-%d %H:%M:%S')\n",
        "  forecast_data['date'] = forecast_data['created_at'].dt.date\n",
        "  forecast_data = forecast_data[forecast_data['channel_id'].isin(metadata['channel_id'])].reset_index(drop=True)\n",
        "  channel_max_dates = forecast_data.groupby('channel_id').apply(lambda x: x['created_at'].max())\n",
        "\n",
        "\n",
        "  ### Set this as a list of chanels to be used. Can be read from a file.\n",
        "  use_channels = channel_max_dates[channel_max_dates > pd.to_datetime('2020-07-12')].index.tolist()\n",
        "  \n",
        "  \n",
        "  forecast_data = forecast_data[forecast_data['channel_id'].isin(use_channels)]\n",
        "\n",
        "  return forecast_data\n",
        "\n",
        "\n",
        "def preprocess_metadata(METADATA_PATH):\n",
        "\n",
        "  metadata = pd.read_csv('meta.csv')\n",
        "  fts = [c for c in metadata if c not in ['chan_id']]\n",
        "  cat_fts = metadata[fts].select_dtypes('object').columns.tolist()\n",
        "  metadata[cat_fts] = metadata[cat_fts].apply(lambda x: pd.factorize(x)[0])\n",
        "  metadata = metadata.rename({'chan_id': 'channel_id'}, axis=1)\n",
        "  drop_fts = ['loc_ref', 'chan_ref', 'loc_mob_stat', 'airqo_name', 'district_lookup', 'county_lookup', 'coords', 'loc_start_date', 'loc_end_date', 'gmaps_link',\n",
        "              'OSM_link', 'nearby_sources', 'geometry', 'geometry_43', 'event_logging_link']\n",
        "  metadata = metadata.drop(drop_fts, axis=1)\n",
        "  metadata.to_csv('metadata_to_use.csv', index = False)\n",
        "\n",
        "  return metadata\n",
        "\n",
        "\n",
        "def get_boundary_layer_mapper(BOUNDARY_LAYER_PATH):\n",
        "  \n",
        "  boundary_layer = pd.read_csv('boundary_layer.csv')\n",
        "  boundary_layer_mapper = pd.Series(index = boundary_layer['hour'], data = boundary_layer['height'])  \n",
        "  \n",
        "  return boundary_layer_mapper"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H203XBUOTmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### These constants must be set before running\n",
        "\n",
        "### Prediction will start from this date-hour\n",
        "TEST_DATE_HOUR_START = pd.to_datetime('2020-07-11 09:00:00')\n",
        "\n",
        "### Prediction will end at this date-hour\n",
        "TEST_DATE_HOUR_END = pd.to_datetime('2020-07-12 08:00:00')\n",
        "\n",
        "### Training will start at this date-hour\n",
        "TRAIN_DATE_HOUR_START = pd.to_datetime('2019-06-01 00:00:00')\n",
        "\n",
        "### Training will end at this date-hour\n",
        "TRAIN_DATE_HOUR_END = pd.to_datetime('2020-07-11 08:00:00')\n",
        "\n",
        "\n",
        "### Boolean value to indicate whether to train or only predict\n",
        "TRAIN_MODEL_NOW = True\n",
        "TARGET_COL = 'pm2_5'\n",
        "\n",
        "########### PATHS #############\n",
        "\n",
        "### These paths must be set\n",
        "FORECAST_DATA_PATH = 'Zindi_PM2_5_forecast_data.csv'\n",
        "METADATA_PATH = 'meta.csv'\n",
        "BOUNDARY_LAYER_PATH = 'boundary_layer.csv'\n",
        "\n",
        "### Trained Model Path. Not required if TRAIN_MODEL_NOW = True\n",
        "TRAIN_MODEL_PATH = 'model.pkl'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJdW_XmadKqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Other Constants. Do not modify them\n",
        "\n",
        "#### Does need to be changed for now. Indicates which channels to predict. \n",
        "CHANNELS_TO_PREDICT_PATH = 'all_channels.pkl'\n",
        "N_HRS_BACK = 24 \n",
        "SEQ_LEN = 24\n",
        "ROLLING_SEQ_LEN = 24*90\n",
        "MAX_LAGS = N_HRS_BACK + max(ROLLING_SEQ_LEN, SEQ_LEN) + 48 # Extra 48 or 2 days for safety\n",
        "TEST_LAG_LAST_DATE_HOUR = TEST_DATE_HOUR_START - pd.Timedelta(hours = MAX_LAGS)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qevRQaKsSjEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1e19046c-d719-46fb-8896-93076d861320"
      },
      "source": [
        "%%time\n",
        "\n",
        "#### Required to preprocess some of the files\n",
        "\n",
        "metadata = preprocess_metadata(METADATA_PATH)\n",
        "\n",
        "#### Later preprocessed forecast data can be saved, so as to avoid preprocessing it again and again. Can add it separately to train and test pipelines to save memory\n",
        "forecast_data = preprocess_forecast_data(FORECAST_DATA_PATH)\n",
        "\n",
        "boundary_layer_mapper = get_boundary_layer_mapper(BOUNDARY_LAYER_PATH)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 46.3 s, sys: 3.98 s, total: 50.3 s\n",
            "Wall time: 50.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5PpYY1UIlH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_agg_channel_data_train(chan_num, freq='1H'):\n",
        "  '''\n",
        "  Get Hourly Aggregates using Mean of the Data for training.\n",
        "  '''\n",
        "\n",
        "  chan = train_forecast_data[train_forecast_data['channel_id'] == chan_num]\n",
        "  chan = chan.sort_values(by = 'created_at')[['created_at', 'pm2_5']].set_index('created_at')\n",
        "  chan = chan.interpolate('time', limit_direction='both')\n",
        "\n",
        "  chan_agg = chan.resample(freq).mean().reset_index()\n",
        "  chan_agg['channel_id'] = chan_num\n",
        "\n",
        "  return chan_agg\n",
        "\n",
        "def get_agg_channel_data_test(chan_num, freq='1H'):\n",
        "  '''\n",
        "  Get Hourly Aggregates using Mean of the Data for testing.\n",
        "  '''\n",
        "\n",
        "  chan = test_forecast_data[test_forecast_data['channel_id'] == chan_num]\n",
        "  chan = chan.sort_values(by = 'created_at')[['created_at', 'pm2_5']].set_index('created_at')\n",
        "  chan = chan.interpolate('time', limit_direction='both')\n",
        "\n",
        "  chan_agg = chan.resample(freq).mean().reset_index()\n",
        "  chan_agg['channel_id'] = chan_num\n",
        "\n",
        "  return chan_agg\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YylIh0VMRzFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_train():\n",
        "\n",
        "  ### Aggregate data every 1 hour using mean\n",
        "  all_channels = train_forecast_data['channel_id'].unique()\n",
        "  joblib.dump(all_channels, 'all_channels.pkl')\n",
        "\n",
        "  op = Parallel(n_jobs = -1)(delayed(get_agg_channel_data_train)(chan_num, freq='1H') for chan_num in all_channels)\n",
        "  train = pd.concat(op, axis=0).reset_index(drop=True)[train_forecast_data.columns.tolist()]\n",
        "  train = train.groupby('channel_id').apply(lambda x: x.set_index('created_at')['pm2_5'].interpolate('time', limit_direction = 'both')).reset_index()\n",
        "\n",
        "  return train"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIvZEhN91Ah2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lag_features(df_tmp):\n",
        "\n",
        "  df_tmp = df_tmp.sort_values(by='created_at')\n",
        "  \n",
        "  ### Shift Features\n",
        "  df_tmp = df_tmp.assign(**{\n",
        "      f'{TARGET_COL} (t-{t})': df_tmp.groupby('channel_id')[TARGET_COL].shift(t)\n",
        "      for t in range(N_HRS_BACK + SEQ_LEN -1, N_HRS_BACK - 1, -1)\n",
        "  })\n",
        "\n",
        "  FIRST_SHIFT_COL = f'{TARGET_COL} (t-{N_HRS_BACK})'\n",
        "  \n",
        "  ### Rolling Features\n",
        "  periods  = [4, 12, 24, 48, 24 * 7, 24 * 14, 24 * 31]\n",
        "\n",
        "  for agg_func in ['min', 'max', 'mean', 'std']:\n",
        "     df_tmp = df_tmp.assign(**{\n",
        "      f'rolling_{agg_func}_{period}_hrs': df_tmp.groupby('channel_id')[FIRST_SHIFT_COL].transform(lambda x: x.rolling(period, min_periods=1).agg(agg_func))\n",
        "      for period in periods\n",
        "    })\n",
        "\n",
        "  df_tmp = df_tmp.assign(**{\n",
        "      f'rolling_range_{period}_hrs': df_tmp[f'rolling_max_{period}_hrs'] - df_tmp[f'rolling_min_{period}_hrs']\n",
        "      for period in periods\n",
        "  })\n",
        "\n",
        "  df_tmp = df_tmp.sort_index()\n",
        "\n",
        "  return df_tmp\n",
        "\n",
        "def get_other_features(df_tmp):\n",
        "  \n",
        "  D_COL = 'created_at'\n",
        "  for attr in ['hour', 'day', 'dayofweek', 'month', 'is_month_start', 'is_month_end', 'week', 'year']:\n",
        "      df_tmp[f'{D_COL}_{attr}'] = getattr(df_tmp[D_COL].dt, attr)\n",
        "\n",
        "  df_tmp['boundary_layer_height'] = df_tmp['created_at_hour'].map(boundary_layer_mapper)\n",
        "  df_tmp['chan_id'] = df_tmp['channel_id']\n",
        "  df_tmp = pd.get_dummies(df_tmp, columns=['chan_id'])\n",
        "\n",
        "  df_tmp = pd.merge(df_tmp, metadata, on = 'channel_id', how = 'left')\n",
        "  return df_tmp\n",
        "\n",
        "def preprocess_df(df_tmp, is_test = False):\n",
        "\n",
        "  df_tmp = get_lag_features(df_tmp)\n",
        "  df_tmp = get_other_features(df_tmp)\n",
        "\n",
        "  return df_tmp"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOC0GhadWNlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train):\n",
        "\n",
        "  features = [c for c in train.columns if c not in [\"created_at\",  \"pm2_5\", \"channel_id\"]]\n",
        "  TARGET_COL = \"pm2_5\"\n",
        "\n",
        "  trn = train.groupby('channel_id').apply(lambda x: x[:-24*7]).reset_index(drop=True)\n",
        "  val = train.groupby('channel_id').apply(lambda x: x[-24*7:]).reset_index(drop=True)\n",
        "  y_trn, y_val  = trn[TARGET_COL], val[TARGET_COL]\n",
        "\n",
        "  clf = LGBMRegressor(n_estimators=5000, learning_rate=0.05, colsample_bytree=0.4, reg_alpha=0, reg_lambda=1, max_depth=-1, random_state=1)\n",
        "  clf.fit(trn[features], y_trn, eval_set = [(val[features], y_val)], verbose=50, early_stopping_rounds=150, eval_metric='rmse')\n",
        "\n",
        "  val_preds = clf.predict(val[features])\n",
        "  rmse_val = mean_squared_error(val[TARGET_COL], val_preds) ** 0.5\n",
        "  print(f'Validation RMSE is {rmse_val}')\n",
        "\n",
        "  best_iter = clf.best_iteration_\n",
        "  clf = LGBMRegressor(n_estimators=best_iter, learning_rate=0.05, colsample_bytree=0.4, reg_alpha=2, reg_lambda=1, max_depth=-1, random_state=1)\n",
        "  clf.fit(train[features], train[TARGET_COL])\n",
        "\n",
        "  return clf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoePDGjbR_A8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "b59051b6-7292-4ef2-8879-6ce78273444d"
      },
      "source": [
        "%%time\n",
        "\n",
        "if TRAIN_MODEL_NOW == True:\n",
        "  train_forecast_data = forecast_data[(forecast_data['created_at'] >= TRAIN_DATE_HOUR_START) & (forecast_data['created_at'] <= TRAIN_DATE_HOUR_END)].drop('date', axis=1)\n",
        "  train = make_train()\n",
        "  train = preprocess_df(train, is_test = False)\n",
        "  clf = train_model(train)\n",
        "\n",
        "  joblib.dump(clf, 'model.pkl')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 150 rounds.\n",
            "[50]\tvalid_0's l2: 623.035\tvalid_0's rmse: 24.9607\n",
            "[100]\tvalid_0's l2: 594.518\tvalid_0's rmse: 24.3827\n",
            "[150]\tvalid_0's l2: 586.664\tvalid_0's rmse: 24.2211\n",
            "[200]\tvalid_0's l2: 581.809\tvalid_0's rmse: 24.1207\n",
            "[250]\tvalid_0's l2: 581.896\tvalid_0's rmse: 24.1225\n",
            "[300]\tvalid_0's l2: 579.714\tvalid_0's rmse: 24.0773\n",
            "[350]\tvalid_0's l2: 579.396\tvalid_0's rmse: 24.0707\n",
            "[400]\tvalid_0's l2: 578.313\tvalid_0's rmse: 24.0481\n",
            "[450]\tvalid_0's l2: 578.045\tvalid_0's rmse: 24.0426\n",
            "[500]\tvalid_0's l2: 576.035\tvalid_0's rmse: 24.0007\n",
            "[550]\tvalid_0's l2: 576.352\tvalid_0's rmse: 24.0073\n",
            "[600]\tvalid_0's l2: 574.774\tvalid_0's rmse: 23.9744\n",
            "[650]\tvalid_0's l2: 574.715\tvalid_0's rmse: 23.9732\n",
            "[700]\tvalid_0's l2: 574.621\tvalid_0's rmse: 23.9712\n",
            "[750]\tvalid_0's l2: 575.023\tvalid_0's rmse: 23.9796\n",
            "[800]\tvalid_0's l2: 575.054\tvalid_0's rmse: 23.9803\n",
            "Early stopping, best iteration is:\n",
            "[695]\tvalid_0's l2: 574.048\tvalid_0's rmse: 23.9593\n",
            "Validation RMSE is 23.95929781343525\n",
            "CPU times: user 4min 42s, sys: 4.14 s, total: 4min 46s\n",
            "Wall time: 1min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbqfHkO1aQhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_24hrs_predictions():\n",
        "\n",
        "  clf = joblib.load(TRAIN_MODEL_PATH)\n",
        "  all_channels = joblib.load('all_channels.pkl')\n",
        "  op = Parallel(n_jobs = -1)(delayed(get_agg_channel_data_test)(chan_num, freq='1H') for chan_num in all_channels)\n",
        "  test = pd.concat(op, axis=0).reset_index(drop=True)[test_forecast_data.columns.tolist()]\n",
        "  \n",
        "  test = test.groupby('channel_id').apply(lambda x: x.set_index('created_at')['pm2_5'].interpolate('time', limit_direction = 'both')).reset_index()\n",
        "\n",
        "  test = preprocess_df(test, is_test = True)\n",
        "  test = test[(test['created_at'] >= TEST_DATE_HOUR_START) & (test['created_at'] <= TEST_DATE_HOUR_END) ]\n",
        "\n",
        "  test_orig = test[[\"created_at\",  \"pm2_5\", \"channel_id\"]].copy()\n",
        "\n",
        "  features = [c for c in train.columns if c not in [\"created_at\",  \"pm2_5\", \"channel_id\"]]\n",
        "  test_preds = clf.predict(test[features])\n",
        "  \n",
        "  test_orig['preds'] = test_preds\n",
        "\n",
        "  return test_orig\n",
        "\n",
        "def get_predictions_for_channel(next_24_hrs, chan_num):\n",
        "  return next_24_hrs[next_24_hrs['channel_id'] == chan_num]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcddFvHseG-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for f in [TRAIN_MODEL_PATH, CHANNELS_TO_PREDICT_PATH]:\n",
        "  if not os.path.isfile(f):\n",
        "    raise Exception(f'{f} not present. Please recheck the file path.')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QdJ5PBebGNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "0a24c282-6e42-4ea2-9bc7-1a68192b6e59"
      },
      "source": [
        "### Predicting Next 24 hours for channels\n",
        "\n",
        "test_forecast_data = forecast_data[(forecast_data['created_at'] >= TEST_LAG_LAST_DATE_HOUR) & (forecast_data['created_at'] <= TEST_DATE_HOUR_END + pd.Timedelta(days=2))].drop('date', axis=1)\n",
        "next_24hrs_predictions = get_next_24hrs_predictions()\n",
        "print(mean_squared_error(next_24hrs_predictions[TARGET_COL], next_24hrs_predictions['preds']) ** 0.5)\n",
        "get_predictions_for_channel(next_24hrs_predictions, 672528)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.66220148727405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>pm2_5</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2232</th>\n",
              "      <td>2020-07-11 09:00:00</td>\n",
              "      <td>45.660000</td>\n",
              "      <td>672528</td>\n",
              "      <td>54.395179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2233</th>\n",
              "      <td>2020-07-11 10:00:00</td>\n",
              "      <td>43.778750</td>\n",
              "      <td>672528</td>\n",
              "      <td>48.286154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2234</th>\n",
              "      <td>2020-07-11 11:00:00</td>\n",
              "      <td>43.269535</td>\n",
              "      <td>672528</td>\n",
              "      <td>45.622670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2235</th>\n",
              "      <td>2020-07-11 12:00:00</td>\n",
              "      <td>58.042791</td>\n",
              "      <td>672528</td>\n",
              "      <td>48.065547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2236</th>\n",
              "      <td>2020-07-11 13:00:00</td>\n",
              "      <td>63.115349</td>\n",
              "      <td>672528</td>\n",
              "      <td>47.478797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2237</th>\n",
              "      <td>2020-07-11 14:00:00</td>\n",
              "      <td>55.516579</td>\n",
              "      <td>672528</td>\n",
              "      <td>47.888279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2238</th>\n",
              "      <td>2020-07-11 15:00:00</td>\n",
              "      <td>49.266818</td>\n",
              "      <td>672528</td>\n",
              "      <td>47.285235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2239</th>\n",
              "      <td>2020-07-11 16:00:00</td>\n",
              "      <td>45.071395</td>\n",
              "      <td>672528</td>\n",
              "      <td>47.925711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2240</th>\n",
              "      <td>2020-07-11 17:00:00</td>\n",
              "      <td>45.014545</td>\n",
              "      <td>672528</td>\n",
              "      <td>52.060151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2241</th>\n",
              "      <td>2020-07-11 18:00:00</td>\n",
              "      <td>58.552326</td>\n",
              "      <td>672528</td>\n",
              "      <td>62.153624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2242</th>\n",
              "      <td>2020-07-11 19:00:00</td>\n",
              "      <td>64.758182</td>\n",
              "      <td>672528</td>\n",
              "      <td>108.478410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2243</th>\n",
              "      <td>2020-07-11 20:00:00</td>\n",
              "      <td>99.916744</td>\n",
              "      <td>672528</td>\n",
              "      <td>117.910210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2244</th>\n",
              "      <td>2020-07-11 21:00:00</td>\n",
              "      <td>77.692308</td>\n",
              "      <td>672528</td>\n",
              "      <td>110.022813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2245</th>\n",
              "      <td>2020-07-11 22:00:00</td>\n",
              "      <td>80.480000</td>\n",
              "      <td>672528</td>\n",
              "      <td>100.787158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2246</th>\n",
              "      <td>2020-07-11 23:00:00</td>\n",
              "      <td>78.427560</td>\n",
              "      <td>672528</td>\n",
              "      <td>92.949644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2247</th>\n",
              "      <td>2020-07-12 00:00:00</td>\n",
              "      <td>76.375119</td>\n",
              "      <td>672528</td>\n",
              "      <td>84.885468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2248</th>\n",
              "      <td>2020-07-12 01:00:00</td>\n",
              "      <td>74.322679</td>\n",
              "      <td>672528</td>\n",
              "      <td>78.166483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2249</th>\n",
              "      <td>2020-07-12 02:00:00</td>\n",
              "      <td>72.270238</td>\n",
              "      <td>672528</td>\n",
              "      <td>74.538602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2250</th>\n",
              "      <td>2020-07-12 03:00:00</td>\n",
              "      <td>70.217798</td>\n",
              "      <td>672528</td>\n",
              "      <td>69.478291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2251</th>\n",
              "      <td>2020-07-12 04:00:00</td>\n",
              "      <td>68.165357</td>\n",
              "      <td>672528</td>\n",
              "      <td>71.487012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2252</th>\n",
              "      <td>2020-07-12 05:00:00</td>\n",
              "      <td>66.112917</td>\n",
              "      <td>672528</td>\n",
              "      <td>69.918578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2253</th>\n",
              "      <td>2020-07-12 06:00:00</td>\n",
              "      <td>64.060476</td>\n",
              "      <td>672528</td>\n",
              "      <td>66.783360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2254</th>\n",
              "      <td>2020-07-12 07:00:00</td>\n",
              "      <td>62.008036</td>\n",
              "      <td>672528</td>\n",
              "      <td>69.051068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2255</th>\n",
              "      <td>2020-07-12 08:00:00</td>\n",
              "      <td>59.955595</td>\n",
              "      <td>672528</td>\n",
              "      <td>63.898019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              created_at      pm2_5  channel_id       preds\n",
              "2232 2020-07-11 09:00:00  45.660000      672528   54.395179\n",
              "2233 2020-07-11 10:00:00  43.778750      672528   48.286154\n",
              "2234 2020-07-11 11:00:00  43.269535      672528   45.622670\n",
              "2235 2020-07-11 12:00:00  58.042791      672528   48.065547\n",
              "2236 2020-07-11 13:00:00  63.115349      672528   47.478797\n",
              "2237 2020-07-11 14:00:00  55.516579      672528   47.888279\n",
              "2238 2020-07-11 15:00:00  49.266818      672528   47.285235\n",
              "2239 2020-07-11 16:00:00  45.071395      672528   47.925711\n",
              "2240 2020-07-11 17:00:00  45.014545      672528   52.060151\n",
              "2241 2020-07-11 18:00:00  58.552326      672528   62.153624\n",
              "2242 2020-07-11 19:00:00  64.758182      672528  108.478410\n",
              "2243 2020-07-11 20:00:00  99.916744      672528  117.910210\n",
              "2244 2020-07-11 21:00:00  77.692308      672528  110.022813\n",
              "2245 2020-07-11 22:00:00  80.480000      672528  100.787158\n",
              "2246 2020-07-11 23:00:00  78.427560      672528   92.949644\n",
              "2247 2020-07-12 00:00:00  76.375119      672528   84.885468\n",
              "2248 2020-07-12 01:00:00  74.322679      672528   78.166483\n",
              "2249 2020-07-12 02:00:00  72.270238      672528   74.538602\n",
              "2250 2020-07-12 03:00:00  70.217798      672528   69.478291\n",
              "2251 2020-07-12 04:00:00  68.165357      672528   71.487012\n",
              "2252 2020-07-12 05:00:00  66.112917      672528   69.918578\n",
              "2253 2020-07-12 06:00:00  64.060476      672528   66.783360\n",
              "2254 2020-07-12 07:00:00  62.008036      672528   69.051068\n",
              "2255 2020-07-12 08:00:00  59.955595      672528   63.898019"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}